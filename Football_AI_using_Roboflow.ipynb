{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markoo26/thehappymountain/blob/main/Football_AI_using_Roboflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFS_cSAqNKHi"
      },
      "source": [
        "# ðŸ Football AI\n",
        "\n",
        "---\n",
        "\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/roboflow/sports)\n",
        "[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/camera-calibration-sports-computer-vision/)\n",
        "[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=aBVGKoNZQUw)\n",
        "\n",
        "Let's build a Football AI system to dig deeper into match stats! We'll use computer vision and machine learning to track players, determine which team is which, and even calculate stuff like ball possession and speed. This tutorial is perfect if you want to get hands-on with sports analytics and see how AI can take your football analysis to the next level.\n",
        "\n",
        "![football AI diagram](https://media.roboflow.com/notebooks/examples/football-ai-diagram.png)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ðŸ”‘ Load API keys for Huggingface and Roboflow + setup env variables\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "os.environ[\"ROBOFLOW_API_KEY\"] = userdata.get(\"ROBOFLOW_API_KEY\")\n",
        "os.environ[\"ONNXRUNTIME_EXECUTION_PROVIDERS\"] = \"[CUDAExecutionProvider]\"\n"
      ],
      "metadata": {
        "id": "WQX_3KmAczkW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8qxMYQZmXgz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ“º Make sure Nvidia CUDA card is accessible\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgZIoz0YTvV-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title âš’ï¸ Install dependencies - gdown and inference-gpu package\n",
        "#@markdown Inference-GPU will be used to run object detection and keypoint detection models efficiently on GPU\n",
        "!pip install -q gdown inference-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcuIp4oXBClh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ðŸŸï¸ Install Roboflow's sports repository directly from github\n",
        "!pip install -q git+https://github.com/roboflow/sports.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWAljqbWYJPX"
      },
      "source": [
        "**Note:** Let's make sure we have the latest features in the supervision library by installing version `0.23.0` or higher."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ðŸŸï¸ Check version of supervision and make sure it's 0.23 or higher\n",
        "!pip list | grep supervision"
      ],
      "metadata": {
        "id": "TOk2rwjbckZN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boWlJuuMUfom"
      },
      "outputs": [],
      "source": [
        "#@title ðŸšš Download MP4 files from Google Drive (Bundesliga Data Shootout)\n",
        "!gdown -O \"0bfacc_0.mp4\" \"https://drive.google.com/uc?id=12TqauVZ9tLAv8kWxTTBFWtgt2hNQ4_ZF\"\n",
        "!gdown -O \"2e57b9_0.mp4\" \"https://drive.google.com/uc?id=19PGw55V8aA6GZu5-Aac5_9mCy3fNxmEf\"\n",
        "!gdown -O \"08fd33_0.mp4\" \"https://drive.google.com/uc?id=1OG8K6wqUw9t7lp9ms1M48DxRhwTYciK-\"\n",
        "!gdown -O \"573e61_0.mp4\" \"https://drive.google.com/uc?id=1yYPKuXbHsCxqjA9G-S6aeR2Kcnos8RPU\"\n",
        "!gdown -O \"121364_0.mp4\" \"https://drive.google.com/uc?id=1vVwjW1dE1drIdd4ZSILfbCGPD4weoNiu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ðŸ›» Imports\n",
        "\n",
        "# === Standard Library ===\n",
        "import base64\n",
        "from collections import deque\n",
        "from io import BytesIO\n",
        "import os\n",
        "import pprint\n",
        "\n",
        "# === Typing ===\n",
        "from typing import Dict, List, Optional, Union\n",
        "\n",
        "# === Third-Party Libraries ===\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import umap\n",
        "\n",
        "from PIL import Image as PIL_Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.cluster import KMeans\n",
        "import plotly.graph_objects as go\n",
        "from transformers import AutoProcessor, SiglipVisionModel\n",
        "import supervision as sv\n",
        "from IPython.core.display import display, HTML, Image\n",
        "from more_itertools import chunked\n",
        "\n",
        "# === Google Colab ===\n",
        "from google.colab import userdata\n",
        "\n",
        "# === Local / Project Imports ===\n",
        "from inference import get_model\n",
        "from sports.common.team import TeamClassifier\n",
        "from sports.common.view import ViewTransformer\n",
        "from sports.annotators.soccer import (\n",
        "    draw_pitch,\n",
        "    draw_points_on_pitch,\n",
        "    draw_paths_on_pitch,\n",
        "    draw_pitch_voronoi_diagram,\n",
        ")\n",
        "from sports.configs.soccer import SoccerPitchConfiguration"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XVs4m1gSmliK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Rq5-SX9WfsY",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title âš½ Setup ONNX Runtime and choose Player Detection Model\n",
        "\n",
        "PLAYER_DETECTION_MODEL_ID = \"football-players-detection-3zvbc/11\" #@param {type:'string'}\n",
        "PLAYER_DETECTION_MODEL = get_model(model_id=PLAYER_DETECTION_MODEL_ID, api_key=os.environ[\"ROBOFLOW_API_KEY\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zz-hd4mdZ4UD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ”¬ Preview frame of all videos\n",
        "FILENAMES = [\"0bfacc_0.mp4\", \"2e57b9_0.mp4\", \"08fd33_0.mp4\", \"573e61_0.mp4\", \"121364_0.mp4\"]\n",
        "\n",
        "for SOURCE_VIDEO_PATH in FILENAMES:\n",
        "    print(f\"Displaying a frame from {SOURCE_VIDEO_PATH}\")\n",
        "    frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "    frame = next(frame_generator)\n",
        "\n",
        "    sv.plot_image(frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duGEOk-j4n4i",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ—’ï¸ Annotate players, goalkeepers and referee on all clips\n",
        "\n",
        "FILENAMES = [\"0bfacc_0.mp4\", \"2e57b9_0.mp4\", \"08fd33_0.mp4\", \"573e61_0.mp4\", \"121364_0.mp4\"]\n",
        "\n",
        "for SOURCE_VIDEO_PATH in FILENAMES:\n",
        "    print(f\"Annotating a frame from {SOURCE_VIDEO_PATH}\")\n",
        "\n",
        "    box_annotator = sv.BoxAnnotator(\n",
        "        color=sv.ColorPalette.from_hex(['#FF8C00', '#00BFFF', '#FF1493', '#FFD700']),\n",
        "        thickness=2\n",
        "    )\n",
        "    label_annotator = sv.LabelAnnotator(\n",
        "        color=sv.ColorPalette.from_hex(['#FF8C00', '#00BFFF', '#FF1493', '#FFD700']),\n",
        "        text_color=sv.Color.from_hex('#000000')\n",
        "    )\n",
        "\n",
        "    frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "    frame = next(frame_generator)\n",
        "\n",
        "    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
        "    detections = sv.Detections.from_inference(result)\n",
        "\n",
        "    labels = [\n",
        "        f\"{class_name} {confidence:.2f}\"\n",
        "        for class_name, confidence\n",
        "        in zip(detections['class_name'], detections.confidence)\n",
        "    ]\n",
        "\n",
        "    annotated_frame = frame.copy()\n",
        "    annotated_frame = box_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=detections)\n",
        "    annotated_frame = label_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=detections,\n",
        "        labels=labels)\n",
        "\n",
        "    sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWoDdvikt4TR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ðŸŽ® Video game style visualization\n",
        "\n",
        "FILENAMES = [\"0bfacc_0.mp4\", \"2e57b9_0.mp4\", \"08fd33_0.mp4\", \"573e61_0.mp4\", \"121364_0.mp4\"]\n",
        "\n",
        "for SOURCE_VIDEO_PATH in FILENAMES:\n",
        "    print(f\"Displaying a frame and performing video game style visualisation from {SOURCE_VIDEO_PATH}\")\n",
        "    BALL_ID = 0\n",
        "\n",
        "    ellipse_annotator = sv.EllipseAnnotator(\n",
        "        color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
        "        thickness=2\n",
        "    )\n",
        "    triangle_annotator = sv.TriangleAnnotator(\n",
        "        color=sv.Color.from_hex('#FFD700'),\n",
        "        base=25,\n",
        "        height=21,\n",
        "        outline_thickness=1\n",
        "    )\n",
        "\n",
        "    frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "    frame = next(frame_generator)\n",
        "\n",
        "    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
        "    detections = sv.Detections.from_inference(result)\n",
        "\n",
        "    ball_detections = detections[detections.class_id == BALL_ID]\n",
        "    ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
        "\n",
        "    all_detections = detections[detections.class_id != BALL_ID]\n",
        "    all_detections = all_detections.with_nms(threshold=0.5, class_agnostic=True)\n",
        "    all_detections.class_id -= 1\n",
        "\n",
        "    annotated_frame = frame.copy()\n",
        "    annotated_frame = ellipse_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=all_detections)\n",
        "    annotated_frame = triangle_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=ball_detections)\n",
        "\n",
        "    sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ixZlM06Gmae",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ§‘â€ðŸ¤â€ðŸ§‘ Player Tracking\n",
        "\n",
        "FILENAMES = [\"0bfacc_0.mp4\", \"2e57b9_0.mp4\", \"08fd33_0.mp4\", \"573e61_0.mp4\", \"121364_0.mp4\"]\n",
        "\n",
        "for SOURCE_VIDEO_PATH in FILENAMES:\n",
        "    print(f\"Displaying a frame from {SOURCE_VIDEO_PATH}\")\n",
        "\n",
        "    BALL_ID = 0\n",
        "\n",
        "    ellipse_annotator = sv.EllipseAnnotator(\n",
        "        color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
        "        thickness=2\n",
        "    )\n",
        "    label_annotator = sv.LabelAnnotator(\n",
        "        color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
        "        text_color=sv.Color.from_hex('#000000'),\n",
        "        text_position=sv.Position.BOTTOM_CENTER\n",
        "    )\n",
        "    triangle_annotator = sv.TriangleAnnotator(\n",
        "        color=sv.Color.from_hex('#FFD700'),\n",
        "        base=25,\n",
        "        height=21,\n",
        "        outline_thickness=1\n",
        "    )\n",
        "\n",
        "    tracker = sv.ByteTrack()\n",
        "    tracker.reset()\n",
        "\n",
        "    frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "    frame = next(frame_generator)\n",
        "\n",
        "    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
        "    detections = sv.Detections.from_inference(result)\n",
        "\n",
        "    ball_detections = detections[detections.class_id == BALL_ID]\n",
        "    ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
        "\n",
        "    all_detections = detections[detections.class_id != BALL_ID]\n",
        "    all_detections = all_detections.with_nms(threshold=0.5, class_agnostic=True)\n",
        "    all_detections.class_id -= 1\n",
        "    all_detections = tracker.update_with_detections(detections=all_detections)\n",
        "\n",
        "    labels = [\n",
        "        f\"#{tracker_id}\"\n",
        "        for tracker_id\n",
        "        in all_detections.tracker_id\n",
        "    ]\n",
        "\n",
        "    annotated_frame = frame.copy()\n",
        "    annotated_frame = ellipse_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=all_detections)\n",
        "    annotated_frame = label_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=all_detections,\n",
        "        labels=labels)\n",
        "    annotated_frame = triangle_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=ball_detections)\n",
        "\n",
        "    sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1smkPKfYm00"
      },
      "source": [
        "## âœ¨Methodology of splitting players into teams\n",
        "\n",
        "![football AI diagram](https://media.roboflow.com/notebooks/examples/football-ai-team-clustering.png)\n",
        "\n",
        "The approach described here is as below:\n",
        "\n",
        "**Step 1** - Get crops of individual players using `PLAYER_DETECTION_MODEL`\n",
        "**Step 2** - Use SigLIP (Sigmoid Loss for Language Image Pre-Training) to construct embeddings of length 768 for each crop\n",
        "**Step 3** - Use UMAP (Uniform Manifold Approximation and Projection) to project those to 3-dimensional vectors. This gives the benefit that you can plot the projected data on a 3D plot\n",
        "**Step 4** - Use KMeans to build 2 clusters of teams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr2smM9fMTSO",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ðŸŸ¥ Split players into teams and display crops\n",
        "\n",
        "FILENAMES = [\"0bfacc_0.mp4\", \"2e57b9_0.mp4\", \"08fd33_0.mp4\", \"573e61_0.mp4\", \"121364_0.mp4\"]\n",
        "\n",
        "crops_dict = {}\n",
        "\n",
        "for SOURCE_VIDEO_PATH in FILENAMES:\n",
        "    print(f\"Displaying 100 crops with football players from {SOURCE_VIDEO_PATH}\")\n",
        "\n",
        "    PLAYER_ID = 2\n",
        "    STRIDE = 30\n",
        "\n",
        "    frame_generator = sv.get_video_frames_generator(\n",
        "        source_path=SOURCE_VIDEO_PATH, stride=STRIDE)\n",
        "\n",
        "    crops = []\n",
        "    for frame in tqdm(frame_generator, desc='collecting crops'):\n",
        "        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
        "        detections = sv.Detections.from_inference(result)\n",
        "        detections = detections.with_nms(threshold=0.5, class_agnostic=True)\n",
        "        detections = detections[detections.class_id == PLAYER_ID]\n",
        "        players_crops = [sv.crop_image(frame, xyxy) for xyxy in detections.xyxy]\n",
        "        crops += players_crops\n",
        "\n",
        "    crops_dict[SOURCE_VIDEO_PATH] = crops\n",
        "\n",
        "    sv.plot_images_grid(crops[:100], grid_size=(10, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLcvVFrbOey8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ”¢ Use model to do embeddings of each crop\n",
        "\n",
        "SIGLIP_MODEL_PATH = 'google/siglip-base-patch16-224' #@param {type:'string'}\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "EMBEDDINGS_MODEL = SiglipVisionModel.from_pretrained(SIGLIP_MODEL_PATH).to(DEVICE)\n",
        "EMBEDDINGS_PROCESSOR = AutoProcessor.from_pretrained(SIGLIP_MODEL_PATH)\n",
        "\n",
        "print(f\"SigLIP model: {SIGLIP_MODEL_PATH} downloaded successfully\")\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "FILENAMES = [\"0bfacc_0.mp4\", \"2e57b9_0.mp4\", \"08fd33_0.mp4\", \"573e61_0.mp4\", \"121364_0.mp4\"]\n",
        "\n",
        "data, crops_pillow_dict = {}, {}\n",
        "\n",
        "for SOURCE_VIDEO_PATH in FILENAMES:\n",
        "\n",
        "    print(f\"Embedding the crops from {SOURCE_VIDEO_PATH}\")\n",
        "    embeddings_data = []\n",
        "\n",
        "    crops = [sv.cv2_to_pillow(crop) for crop in crops_dict[SOURCE_VIDEO_PATH]]\n",
        "    crops_pillow_dict[SOURCE_VIDEO_PATH] = crops\n",
        "\n",
        "    batches = chunked(crops, BATCH_SIZE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(batches, desc='embedding extraction'):\n",
        "            inputs = EMBEDDINGS_PROCESSOR(images=batch, return_tensors=\"pt\").to(DEVICE)\n",
        "            outputs = EMBEDDINGS_MODEL(**inputs)\n",
        "            embeddings = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()\n",
        "            embeddings_data.append(embeddings)\n",
        "\n",
        "    data[SOURCE_VIDEO_PATH] = np.concatenate(embeddings_data)\n",
        "\n",
        "shapes_dict = {m: e.shape for m, e in data.items()}\n",
        "\n",
        "print(f\"\\n\\nEmbeddings shape: \")\n",
        "pprint.pprint(shapes_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EXV02O3SWsX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ðŸª„ Using UMAP project embeddings from (N,768) to (N,3) and then use KMeans to do two-cluster (team) division with KMeans\n",
        "\n",
        "movie_clip = '573e61_0.mp4' #@param ['08fd33_0.mp4', '0bfacc_0.mp4', '121364_0.mp4', '2e57b9_0.mp4', '573e61_0.mp4']\n",
        "\n",
        "single_data = data[movie_clip]\n",
        "\n",
        "REDUCER = umap.UMAP(n_components=3)\n",
        "CLUSTERING_MODEL = KMeans(n_clusters=2)\n",
        "projections = REDUCER.fit_transform(single_data)\n",
        "clusters = CLUSTERING_MODEL.fit_predict(projections)\n",
        "\n",
        "\n",
        "\n",
        "def pil_image_to_data_uri(image: Image.Image) -> str:\n",
        "    buffered = BytesIO()\n",
        "    image.save(buffered, format=\"PNG\")\n",
        "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "    return f\"data:image/png;base64,{img_str}\"\n",
        "\n",
        "\n",
        "def display_projections(\n",
        "    labels: np.ndarray,\n",
        "    projections: np.ndarray,\n",
        "    images: List[Image.Image],\n",
        "    show_legend: bool = False,\n",
        "    show_markers_with_text: bool = True\n",
        ") -> None:\n",
        "    image_data_uris = {f\"image_{i}\": pil_image_to_data_uri(image) for i, image in enumerate(images)}\n",
        "    image_ids = np.array([f\"image_{i}\" for i in range(len(images))])\n",
        "\n",
        "    unique_labels = np.unique(labels)\n",
        "    traces = []\n",
        "    for unique_label in unique_labels:\n",
        "        mask = labels == unique_label\n",
        "        customdata_masked = image_ids[mask]\n",
        "        trace = go.Scatter3d(\n",
        "            x=projections[mask][:, 0],\n",
        "            y=projections[mask][:, 1],\n",
        "            z=projections[mask][:, 2],\n",
        "            mode='markers+text' if show_markers_with_text else 'markers',\n",
        "            text=labels[mask],\n",
        "            customdata=customdata_masked,\n",
        "            name=str(unique_label),\n",
        "            marker=dict(size=8),\n",
        "            hovertemplate=\"<b>class: %{text}</b><br>image ID: %{customdata}<extra></extra>\"\n",
        "        )\n",
        "        traces.append(trace)\n",
        "\n",
        "    # Calculate shared range for cube appearance\n",
        "    all_axes = projections\n",
        "    min_val = np.min(all_axes)\n",
        "    max_val = np.max(all_axes)\n",
        "    padding = (max_val - min_val) * 0.05\n",
        "    axis_range = [min_val - padding, max_val + padding]\n",
        "\n",
        "    fig = go.Figure(data=traces)\n",
        "    fig.update_layout(\n",
        "        scene=dict(\n",
        "            xaxis=dict(title='X', range=axis_range),\n",
        "            yaxis=dict(title='Y', range=axis_range),\n",
        "            zaxis=dict(title='Z', range=axis_range),\n",
        "            aspectmode='cube'  # Ensures equal scaling\n",
        "        ),\n",
        "        width=1000,\n",
        "        height=1000,\n",
        "        showlegend=show_legend,\n",
        "    )\n",
        "\n",
        "    plotly_div = fig.to_html(full_html=False, include_plotlyjs=False, div_id=\"scatter-plot-3d\")\n",
        "\n",
        "    javascript_code = f\"\"\"\n",
        "    <script>\n",
        "        function displayImage(imageId) {{\n",
        "            var imageElement = document.getElementById('image-display');\n",
        "            var placeholderText = document.getElementById('placeholder-text');\n",
        "            var imageDataURIs = {image_data_uris};\n",
        "            imageElement.src = imageDataURIs[imageId];\n",
        "            imageElement.style.display = 'block';\n",
        "            placeholderText.style.display = 'none';\n",
        "        }}\n",
        "\n",
        "        var chartElement = document.getElementById('scatter-plot-3d');\n",
        "\n",
        "        chartElement.on('plotly_click', function(data) {{\n",
        "            var customdata = data.points[0].customdata;\n",
        "            displayImage(customdata);\n",
        "        }});\n",
        "    </script>\n",
        "    \"\"\"\n",
        "\n",
        "    html_template = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "        <head>\n",
        "            <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
        "            <style>\n",
        "                #image-container {{\n",
        "                    position: fixed;\n",
        "                    top: 0;\n",
        "                    left: 0;\n",
        "                    width: 200px;\n",
        "                    height: 200px;\n",
        "                    padding: 5px;\n",
        "                    border: 1px solid #ccc;\n",
        "                    background-color: white;\n",
        "                    z-index: 1000;\n",
        "                    box-sizing: border-box;\n",
        "                    display: flex;\n",
        "                    align-items: center;\n",
        "                    justify-content: center;\n",
        "                    text-align: center;\n",
        "                }}\n",
        "                #image-display {{\n",
        "                    width: 100%;\n",
        "                    height: 100%;\n",
        "                    object-fit: contain;\n",
        "                }}\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            {plotly_div}\n",
        "            <div id=\"image-container\">\n",
        "                <img id=\"image-display\" src=\"\" alt=\"Selected image\" style=\"display: none;\" />\n",
        "                <p id=\"placeholder-text\">Click on a data entry to display an image</p>\n",
        "            </div>\n",
        "            {javascript_code}\n",
        "        </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    display(HTML(html_template))\n",
        "print(f\"Displaying projections for {movie_clip}\")\n",
        "display_projections(clusters, projections, crops_pillow_dict[movie_clip])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZG1DorZ8lSQ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title âš™ï¸ Do the same but with TeamClassifier object that encapsulates SigLIP -> UMAP -> KMeans chain\n",
        "\n",
        "FILENAMES = [\"0bfacc_0.mp4\", \"2e57b9_0.mp4\", \"08fd33_0.mp4\", \"573e61_0.mp4\", \"121364_0.mp4\"]\n",
        "\n",
        "team_classifier_dict = {}\n",
        "\n",
        "for SOURCE_VIDEO_PATH in FILENAMES:\n",
        "\n",
        "    PLAYER_ID = 2\n",
        "    STRIDE = 30\n",
        "\n",
        "    frame_generator = sv.get_video_frames_generator(\n",
        "        source_path=SOURCE_VIDEO_PATH, stride=STRIDE)\n",
        "\n",
        "    crops = []\n",
        "    for frame in tqdm(frame_generator, desc='collecting crops'):\n",
        "        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
        "        detections = sv.Detections.from_inference(result)\n",
        "        players_detections = detections[detections.class_id == PLAYER_ID]\n",
        "        players_crops = [sv.crop_image(frame, xyxy) for xyxy in detections.xyxy]\n",
        "        crops += players_crops\n",
        "\n",
        "    team_classifier = TeamClassifier(device=\"cuda\")\n",
        "    team_classifier.fit(crops)\n",
        "    team_classifier_dict[SOURCE_VIDEO_PATH] = team_classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx_4g2DGC1Bd"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ¥… Assign goalkeepers to teams by using average position (centroid) of players from both teams and comparing with goalkeeper's position\n",
        "\n",
        "def resolve_goalkeepers_team_id(\n",
        "    players: sv.Detections,\n",
        "    goalkeepers: sv.Detections\n",
        ") -> np.ndarray:\n",
        "    goalkeepers_xy = goalkeepers.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
        "    players_xy = players.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
        "    team_0_centroid = players_xy[players.class_id == 0].mean(axis=0)\n",
        "    team_1_centroid = players_xy[players.class_id == 1].mean(axis=0)\n",
        "    goalkeepers_team_id = []\n",
        "    for goalkeeper_xy in goalkeepers_xy:\n",
        "        dist_0 = np.linalg.norm(goalkeeper_xy - team_0_centroid)\n",
        "        dist_1 = np.linalg.norm(goalkeeper_xy - team_1_centroid)\n",
        "        goalkeepers_team_id.append(0 if dist_0 < dist_1 else 1)\n",
        "\n",
        "    return np.array(goalkeepers_team_id)\n",
        "\n",
        "\n",
        "FILENAMES = [\"0bfacc_0.mp4\", \"2e57b9_0.mp4\", \"08fd33_0.mp4\", \"573e61_0.mp4\", \"121364_0.mp4\"]\n",
        "\n",
        "for SOURCE_VIDEO_PATH in FILENAMES:\n",
        "\n",
        "    BALL_ID = 0\n",
        "    GOALKEEPER_ID = 1\n",
        "    PLAYER_ID = 2\n",
        "    REFEREE_ID = 3\n",
        "\n",
        "    ellipse_annotator = sv.EllipseAnnotator(\n",
        "        color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
        "        thickness=2\n",
        "    )\n",
        "    label_annotator = sv.LabelAnnotator(\n",
        "        color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
        "        text_color=sv.Color.from_hex('#000000'),\n",
        "        text_position=sv.Position.BOTTOM_CENTER\n",
        "    )\n",
        "    triangle_annotator = sv.TriangleAnnotator(\n",
        "        color=sv.Color.from_hex('#FFD700'),\n",
        "        base=25,\n",
        "        height=21,\n",
        "        outline_thickness=1\n",
        "    )\n",
        "\n",
        "    tracker = sv.ByteTrack()\n",
        "    tracker.reset()\n",
        "\n",
        "    frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "    frame = next(frame_generator)\n",
        "\n",
        "    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
        "    detections = sv.Detections.from_inference(result)\n",
        "\n",
        "    ball_detections = detections[detections.class_id == BALL_ID]\n",
        "    ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
        "\n",
        "    all_detections = detections[detections.class_id != BALL_ID]\n",
        "    all_detections = all_detections.with_nms(threshold=0.5, class_agnostic=True)\n",
        "    all_detections = tracker.update_with_detections(detections=all_detections)\n",
        "\n",
        "    goalkeepers_detections = all_detections[all_detections.class_id == GOALKEEPER_ID]\n",
        "    players_detections = all_detections[all_detections.class_id == PLAYER_ID]\n",
        "    referees_detections = all_detections[all_detections.class_id == REFEREE_ID]\n",
        "\n",
        "    players_crops = [sv.crop_image(frame, xyxy) for xyxy in players_detections.xyxy]\n",
        "    players_detections.class_id = team_classifier.predict(players_crops)\n",
        "\n",
        "    goalkeepers_detections.class_id = resolve_goalkeepers_team_id(\n",
        "        players_detections, goalkeepers_detections)\n",
        "\n",
        "    referees_detections.class_id -= 1\n",
        "\n",
        "    all_detections = sv.Detections.merge([\n",
        "        players_detections, goalkeepers_detections, referees_detections])\n",
        "\n",
        "    labels = [\n",
        "        f\"#{tracker_id}\"\n",
        "        for tracker_id\n",
        "        in all_detections.tracker_id\n",
        "    ]\n",
        "\n",
        "    all_detections.class_id = all_detections.class_id.astype(int)\n",
        "\n",
        "    annotated_frame = frame.copy()\n",
        "    annotated_frame = ellipse_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=all_detections)\n",
        "    annotated_frame = label_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=all_detections,\n",
        "        labels=labels)\n",
        "    annotated_frame = triangle_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=ball_detections)\n",
        "\n",
        "    print(f\"Annotating goalkeeper for {SOURCE_VIDEO_PATH}\")\n",
        "    sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts9f1goyCPEa",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ðŸš¥ Pitch Keypoint detection\n",
        "\n",
        "FIELD_DETECTION_MODEL_ID = \"football-field-detection-f07vi/14\" #@param {type:'string'}\n",
        "FIELD_DETECTION_MODEL = get_model(model_id=FIELD_DETECTION_MODEL_ID, api_key=os.environ['ROBOFLOW_API_KEY'])\n",
        "\n",
        "print(f\"Loaded FIELD_DETECTION_MODEL: {FIELD_DETECTION_MODEL_ID}\")\n",
        "\n",
        "FILENAMES = [\"0bfacc_0.mp4\", \"2e57b9_0.mp4\", \"08fd33_0.mp4\", \"573e61_0.mp4\", \"121364_0.mp4\"]\n",
        "\n",
        "for SOURCE_VIDEO_PATH in FILENAMES:\n",
        "\n",
        "    vertex_annotator = sv.VertexAnnotator(\n",
        "        color=sv.Color.from_hex('#FF1493'),\n",
        "        radius=8)\n",
        "\n",
        "    frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "    frame = next(frame_generator)\n",
        "\n",
        "    result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
        "    key_points = sv.KeyPoints.from_inference(result)\n",
        "\n",
        "    annotated_frame = frame.copy()\n",
        "    annotated_frame = vertex_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        key_points=key_points)\n",
        "\n",
        "    print(f\"Running Field Detection Model against {SOURCE_VIDEO_PATH}\")\n",
        "    sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4XW4_bDGjlF",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ðŸª« Filter out keypoints with low confidence\n",
        "\n",
        "FILENAMES = [\"0bfacc_0.mp4\", \"2e57b9_0.mp4\", \"08fd33_0.mp4\", \"573e61_0.mp4\", \"121364_0.mp4\"]\n",
        "\n",
        "\n",
        "for SOURCE_VIDEO_PATH in FILENAMES:\n",
        "    vertex_annotator = sv.VertexAnnotator(\n",
        "        color=sv.Color.from_hex('#FF1493'),\n",
        "        radius=8)\n",
        "\n",
        "    frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, start=200)\n",
        "    frame = next(frame_generator)\n",
        "\n",
        "    result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
        "    key_points = sv.KeyPoints.from_inference(result)\n",
        "\n",
        "    filter = key_points.confidence[0] > 0.5\n",
        "    frame_reference_points = key_points.xy[0][filter]\n",
        "    frame_reference_key_points = sv.KeyPoints(\n",
        "        xy=frame_reference_points[np.newaxis, ...])\n",
        "\n",
        "    annotated_frame = frame.copy()\n",
        "    annotated_frame = vertex_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        key_points=frame_reference_key_points)\n",
        "    print(f\"Keypoints for {SOURCE_VIDEO_PATH} with ones with low confidence filtered out \")\n",
        "    sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5y4tMWbaiC6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title âšž SoccerPitchConfiguration preview - default idea on how to draw the geometry of soccer pitch\n",
        "\n",
        "CONFIG = SoccerPitchConfiguration()\n",
        "\n",
        "annotated_frame = draw_pitch(CONFIG)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1abHrITPB3Bd"
      },
      "source": [
        "**Note:** It's time to utilize the keypoint pairs located on the camera perspective plane and the football pitch plane. The [sports](https://github.com/roboflow/sports) repository includes a [`ViewTransformer`](https://github.com/roboflow/sports/blob/06053616f1f8a8ae1fa936eb00dcdc2e4f888bb1/sports/common/view.py#L7), which employs homography for perspective transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjoG33jod7M8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ¤– Apply using ViewTransformer the pitch lines\n",
        "\n",
        "FILENAMES = [\"0bfacc_0.mp4\", \"2e57b9_0.mp4\", \"08fd33_0.mp4\", \"573e61_0.mp4\", \"121364_0.mp4\"]\n",
        "\n",
        "\n",
        "for SOURCE_VIDEO_PATH in FILENAMES:\n",
        "\n",
        "    edge_annotator = sv.EdgeAnnotator(\n",
        "        color=sv.Color.from_hex('#00BFFF'),\n",
        "        thickness=2, edges=CONFIG.edges)\n",
        "    vertex_annotator = sv.VertexAnnotator(\n",
        "        color=sv.Color.from_hex('#FF1493'),\n",
        "        radius=8)\n",
        "    vertex_annotator_2 = sv.VertexAnnotator(\n",
        "        color=sv.Color.from_hex('#00BFFF'),\n",
        "        radius=8)\n",
        "\n",
        "    frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, start=200)\n",
        "    frame = next(frame_generator)\n",
        "\n",
        "    result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
        "    key_points = sv.KeyPoints.from_inference(result)\n",
        "\n",
        "    filter = key_points.confidence[0] > 0.5\n",
        "    frame_reference_points = key_points.xy[0][filter]\n",
        "    frame_reference_key_points = sv.KeyPoints(\n",
        "        xy=frame_reference_points[np.newaxis, ...])\n",
        "\n",
        "    pitch_reference_points = np.array(CONFIG.vertices)[filter]\n",
        "\n",
        "    transformer = ViewTransformer(\n",
        "        source=pitch_reference_points,\n",
        "        target=frame_reference_points\n",
        "    )\n",
        "\n",
        "    pitch_all_points = np.array(CONFIG.vertices)\n",
        "    frame_all_points = transformer.transform_points(points=pitch_all_points)\n",
        "\n",
        "    frame_all_key_points = sv.KeyPoints(xy=frame_all_points[np.newaxis, ...])\n",
        "\n",
        "    annotated_frame = frame.copy()\n",
        "    annotated_frame = edge_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        key_points=frame_all_key_points)\n",
        "    annotated_frame = vertex_annotator_2.annotate(\n",
        "        scene=annotated_frame,\n",
        "        key_points=frame_all_key_points)\n",
        "    annotated_frame = vertex_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        key_points=frame_reference_key_points)\n",
        "\n",
        "    print(f\"Applying football pitch grid using the ViewTransformer for {SOURCE_VIDEO_PATH}\")\n",
        "    sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9nRbJnksPyE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title â›¹ï¸ Project ball, players and referees on the pitch\n",
        "\n",
        "# SOURCE_VIDEO_PATH = \"/content/121364_0.mp4\"\n",
        "# PLAYER_ID = 2\n",
        "# STRIDE = 30\n",
        "\n",
        "# frame_generator = sv.get_video_frames_generator(\n",
        "#     source_path=SOURCE_VIDEO_PATH, stride=STRIDE)\n",
        "\n",
        "# crops = []\n",
        "# for frame in tqdm(frame_generator, desc='collecting crops'):\n",
        "#     result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
        "#     detections = sv.Detections.from_inference(result)\n",
        "#     players_detections = detections[detections.class_id == PLAYER_ID]\n",
        "#     players_crops = [sv.crop_image(frame, xyxy) for xyxy in detections.xyxy]\n",
        "#     crops += players_crops\n",
        "\n",
        "# team_classifier = TeamClassifier(device=\"cuda\")\n",
        "# team_classifier.fit(crops)\n",
        "\n",
        "def draw_pitch_voronoi_diagram_2(\n",
        "    config: SoccerPitchConfiguration,\n",
        "    team_1_xy: np.ndarray,\n",
        "    team_2_xy: np.ndarray,\n",
        "    team_1_color: sv.Color = sv.Color.RED,\n",
        "    team_2_color: sv.Color = sv.Color.WHITE,\n",
        "    opacity: float = 0.5,\n",
        "    padding: int = 50,\n",
        "    scale: float = 0.1,\n",
        "    pitch: Optional[np.ndarray] = None\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Draws a Voronoi diagram on a soccer pitch representing the control areas of two\n",
        "    teams with smooth color transitions.\n",
        "\n",
        "    Args:\n",
        "        config (SoccerPitchConfiguration): Configuration object containing the\n",
        "            dimensions and layout of the pitch.\n",
        "        team_1_xy (np.ndarray): Array of (x, y) coordinates representing the positions\n",
        "            of players in team 1.\n",
        "        team_2_xy (np.ndarray): Array of (x, y) coordinates representing the positions\n",
        "            of players in team 2.\n",
        "        team_1_color (sv.Color, optional): Color representing the control area of\n",
        "            team 1. Defaults to sv.Color.RED.\n",
        "        team_2_color (sv.Color, optional): Color representing the control area of\n",
        "            team 2. Defaults to sv.Color.WHITE.\n",
        "        opacity (float, optional): Opacity of the Voronoi diagram overlay.\n",
        "            Defaults to 0.5.\n",
        "        padding (int, optional): Padding around the pitch in pixels.\n",
        "            Defaults to 50.\n",
        "        scale (float, optional): Scaling factor for the pitch dimensions.\n",
        "            Defaults to 0.1.\n",
        "        pitch (Optional[np.ndarray], optional): Existing pitch image to draw the\n",
        "            Voronoi diagram on. If None, a new pitch will be created. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Image of the soccer pitch with the Voronoi diagram overlay.\n",
        "    \"\"\"\n",
        "    if pitch is None:\n",
        "        pitch = draw_pitch(\n",
        "            config=config,\n",
        "            padding=padding,\n",
        "            scale=scale\n",
        "        )\n",
        "\n",
        "    scaled_width = int(config.width * scale)\n",
        "    scaled_length = int(config.length * scale)\n",
        "\n",
        "    voronoi = np.zeros_like(pitch, dtype=np.uint8)\n",
        "\n",
        "    team_1_color_bgr = np.array(team_1_color.as_bgr(), dtype=np.uint8)\n",
        "    team_2_color_bgr = np.array(team_2_color.as_bgr(), dtype=np.uint8)\n",
        "\n",
        "    y_coordinates, x_coordinates = np.indices((\n",
        "        scaled_width + 2 * padding,\n",
        "        scaled_length + 2 * padding\n",
        "    ))\n",
        "\n",
        "    y_coordinates -= padding\n",
        "    x_coordinates -= padding\n",
        "\n",
        "    def calculate_distances(xy, x_coordinates, y_coordinates):\n",
        "        return np.sqrt((xy[:, 0][:, None, None] * scale - x_coordinates) ** 2 +\n",
        "                       (xy[:, 1][:, None, None] * scale - y_coordinates) ** 2)\n",
        "\n",
        "    distances_team_1 = calculate_distances(team_1_xy, x_coordinates, y_coordinates)\n",
        "    distances_team_2 = calculate_distances(team_2_xy, x_coordinates, y_coordinates)\n",
        "\n",
        "    min_distances_team_1 = np.min(distances_team_1, axis=0)\n",
        "    min_distances_team_2 = np.min(distances_team_2, axis=0)\n",
        "\n",
        "    # Increase steepness of the blend effect\n",
        "    steepness = 15  # Increased steepness for sharper transition\n",
        "    distance_ratio = min_distances_team_2 / np.clip(min_distances_team_1 + min_distances_team_2, a_min=1e-5, a_max=None)\n",
        "    blend_factor = np.tanh((distance_ratio - 0.5) * steepness) * 0.5 + 0.5\n",
        "\n",
        "    # Create the smooth color transition\n",
        "    for c in range(3):  # Iterate over the B, G, R channels\n",
        "        voronoi[:, :, c] = (blend_factor * team_1_color_bgr[c] +\n",
        "                            (1 - blend_factor) * team_2_color_bgr[c]).astype(np.uint8)\n",
        "\n",
        "    overlay = cv2.addWeighted(voronoi, opacity, pitch, 1 - opacity, 0)\n",
        "\n",
        "    return overlay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rjfNfzErrSN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ðŸŽ¨ Nice 2D visualizations and others\n",
        "#@markdown Ideas:\n",
        "#@markdown * Validate there are 11 players assigned to team A and 11 players assigned to team B\n",
        "#@markdown * Retry with different setup until those conditions are met\n",
        "#@markdown * Convert football video to animated radar plot\n",
        "\n",
        "FILENAMES = [\"0bfacc_0.mp4\", \"2e57b9_0.mp4\", \"08fd33_0.mp4\", \"573e61_0.mp4\", \"121364_0.mp4\"]\n",
        "\n",
        "\n",
        "for SOURCE_VIDEO_PATH in FILENAMES:\n",
        "\n",
        "    BALL_ID = 0\n",
        "    GOALKEEPER_ID = 1\n",
        "    PLAYER_ID = 2\n",
        "    REFEREE_ID = 3\n",
        "\n",
        "    ellipse_annotator = sv.EllipseAnnotator(\n",
        "        color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
        "        thickness=2\n",
        "    )\n",
        "    label_annotator = sv.LabelAnnotator(\n",
        "        color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
        "        text_color=sv.Color.from_hex('#000000'),\n",
        "        text_position=sv.Position.BOTTOM_CENTER\n",
        "    )\n",
        "    triangle_annotator = sv.TriangleAnnotator(\n",
        "        color=sv.Color.from_hex('#FFD700'),\n",
        "        base=20, height=17\n",
        "    )\n",
        "\n",
        "    tracker = sv.ByteTrack()\n",
        "    tracker.reset()\n",
        "\n",
        "    frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "    frame = next(frame_generator)\n",
        "\n",
        "    # ball, goalkeeper, player, referee detection\n",
        "\n",
        "    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
        "    detections = sv.Detections.from_inference(result)\n",
        "\n",
        "    ball_detections = detections[detections.class_id == BALL_ID]\n",
        "    ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
        "\n",
        "    all_detections = detections[detections.class_id != BALL_ID]\n",
        "    all_detections = all_detections.with_nms(threshold=0.5, class_agnostic=True)\n",
        "    all_detections = tracker.update_with_detections(detections=all_detections)\n",
        "\n",
        "    goalkeepers_detections = all_detections[all_detections.class_id == GOALKEEPER_ID]\n",
        "    players_detections = all_detections[all_detections.class_id == PLAYER_ID]\n",
        "    referees_detections = all_detections[all_detections.class_id == REFEREE_ID]\n",
        "\n",
        "    # team assignment\n",
        "\n",
        "    players_crops = [sv.crop_image(frame, xyxy) for xyxy in players_detections.xyxy]\n",
        "    players_detections.class_id = team_classifier.predict(players_crops)\n",
        "\n",
        "    goalkeepers_detections.class_id = resolve_goalkeepers_team_id(\n",
        "        players_detections, goalkeepers_detections)\n",
        "\n",
        "    referees_detections.class_id -= 1\n",
        "\n",
        "    all_detections = sv.Detections.merge([\n",
        "        players_detections, goalkeepers_detections, referees_detections])\n",
        "\n",
        "    # frame visualization\n",
        "\n",
        "    labels = [\n",
        "        f\"#{tracker_id}\"\n",
        "        for tracker_id\n",
        "        in all_detections.tracker_id\n",
        "    ]\n",
        "\n",
        "    all_detections.class_id = all_detections.class_id.astype(int)\n",
        "\n",
        "    annotated_frame = frame.copy()\n",
        "    annotated_frame = ellipse_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=all_detections)\n",
        "    annotated_frame = label_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=all_detections,\n",
        "        labels=labels)\n",
        "    annotated_frame = triangle_annotator.annotate(\n",
        "        scene=annotated_frame,\n",
        "        detections=ball_detections)\n",
        "\n",
        "    sv.plot_image(annotated_frame)\n",
        "\n",
        "    players_detections = sv.Detections.merge([\n",
        "        players_detections, goalkeepers_detections\n",
        "    ])\n",
        "\n",
        "    # detect pitch key points\n",
        "\n",
        "    result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
        "    key_points = sv.KeyPoints.from_inference(result)\n",
        "\n",
        "    # project ball, players and referies on pitch\n",
        "\n",
        "    filter = key_points.confidence[0] > 0.5\n",
        "    frame_reference_points = key_points.xy[0][filter]\n",
        "    pitch_reference_points = np.array(CONFIG.vertices)[filter]\n",
        "\n",
        "    transformer = ViewTransformer(\n",
        "        source=frame_reference_points,\n",
        "        target=pitch_reference_points\n",
        "    )\n",
        "\n",
        "    frame_ball_xy = ball_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
        "    pitch_ball_xy = transformer.transform_points(points=frame_ball_xy)\n",
        "\n",
        "    players_xy = players_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
        "    pitch_players_xy = transformer.transform_points(points=players_xy)\n",
        "\n",
        "    referees_xy = referees_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
        "    pitch_referees_xy = transformer.transform_points(points=referees_xy)\n",
        "\n",
        "    # visualize video game-style radar view\n",
        "\n",
        "    annotated_frame = draw_pitch(CONFIG)\n",
        "    annotated_frame = draw_points_on_pitch(\n",
        "        config=CONFIG,\n",
        "        xy=pitch_ball_xy,\n",
        "        face_color=sv.Color.WHITE,\n",
        "        edge_color=sv.Color.BLACK,\n",
        "        radius=10,\n",
        "        pitch=annotated_frame)\n",
        "    annotated_frame = draw_points_on_pitch(\n",
        "        config=CONFIG,\n",
        "        xy=pitch_players_xy[players_detections.class_id == 0],\n",
        "        face_color=sv.Color.from_hex('00BFFF'),\n",
        "        edge_color=sv.Color.BLACK,\n",
        "        radius=16,\n",
        "        pitch=annotated_frame)\n",
        "    annotated_frame = draw_points_on_pitch(\n",
        "        config=CONFIG,\n",
        "        xy=pitch_players_xy[players_detections.class_id == 1],\n",
        "        face_color=sv.Color.from_hex('FF1493'),\n",
        "        edge_color=sv.Color.BLACK,\n",
        "        radius=16,\n",
        "        pitch=annotated_frame)\n",
        "    annotated_frame = draw_points_on_pitch(\n",
        "        config=CONFIG,\n",
        "        xy=pitch_referees_xy,\n",
        "        face_color=sv.Color.from_hex('FFD700'),\n",
        "        edge_color=sv.Color.BLACK,\n",
        "        radius=16,\n",
        "        pitch=annotated_frame)\n",
        "\n",
        "    sv.plot_image(annotated_frame)\n",
        "\n",
        "    # visualize voronoi diagram\n",
        "\n",
        "    annotated_frame = draw_pitch(CONFIG)\n",
        "    annotated_frame = draw_pitch_voronoi_diagram(\n",
        "        config=CONFIG,\n",
        "        team_1_xy=pitch_players_xy[players_detections.class_id == 0],\n",
        "        team_2_xy=pitch_players_xy[players_detections.class_id == 1],\n",
        "        team_1_color=sv.Color.from_hex('00BFFF'),\n",
        "        team_2_color=sv.Color.from_hex('FF1493'),\n",
        "        pitch=annotated_frame)\n",
        "\n",
        "    sv.plot_image(annotated_frame)\n",
        "\n",
        "    # visualize voronoi diagram with blend\n",
        "\n",
        "    annotated_frame = draw_pitch(\n",
        "        config=CONFIG,\n",
        "        background_color=sv.Color.WHITE,\n",
        "        line_color=sv.Color.BLACK\n",
        "    )\n",
        "    annotated_frame = draw_pitch_voronoi_diagram_2(\n",
        "        config=CONFIG,\n",
        "        team_1_xy=pitch_players_xy[players_detections.class_id == 0],\n",
        "        team_2_xy=pitch_players_xy[players_detections.class_id == 1],\n",
        "        team_1_color=sv.Color.from_hex('00BFFF'),\n",
        "        team_2_color=sv.Color.from_hex('FF1493'),\n",
        "        pitch=annotated_frame)\n",
        "    annotated_frame = draw_points_on_pitch(\n",
        "        config=CONFIG,\n",
        "        xy=pitch_ball_xy,\n",
        "        face_color=sv.Color.WHITE,\n",
        "        edge_color=sv.Color.WHITE,\n",
        "        radius=8,\n",
        "        thickness=1,\n",
        "        pitch=annotated_frame)\n",
        "    annotated_frame = draw_points_on_pitch(\n",
        "        config=CONFIG,\n",
        "        xy=pitch_players_xy[players_detections.class_id == 0],\n",
        "        face_color=sv.Color.from_hex('00BFFF'),\n",
        "        edge_color=sv.Color.WHITE,\n",
        "        radius=16,\n",
        "        thickness=1,\n",
        "        pitch=annotated_frame)\n",
        "    annotated_frame = draw_points_on_pitch(\n",
        "        config=CONFIG,\n",
        "        xy=pitch_players_xy[players_detections.class_id == 1],\n",
        "        face_color=sv.Color.from_hex('FF1493'),\n",
        "        edge_color=sv.Color.WHITE,\n",
        "        radius=16,\n",
        "        thickness=1,\n",
        "        pitch=annotated_frame)\n",
        "    print(f\"Displaying Voronoi Diagram for {SOURCE_VIDEO_PATH}\")\n",
        "    sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ðŸ§™â€â™‚ï¸ Create animated radar plot GIFs around (âŒš wait 3 mins for each MP4)\n",
        "\n",
        "\n",
        "FILENAMES = [\"0bfacc_0.mp4\", \"2e57b9_0.mp4\", \"08fd33_0.mp4\", \"573e61_0.mp4\", \"121364_0.mp4\"]\n",
        "\n",
        "\n",
        "for SOURCE_VIDEO_PATH in tqdm(FILENAMES):\n",
        "\n",
        "    print(f\"Processing {SOURCE_VIDEO_PATH}\")\n",
        "\n",
        "    BALL_ID = 0\n",
        "    GOALKEEPER_ID = 1\n",
        "    PLAYER_ID = 2\n",
        "    REFEREE_ID = 3\n",
        "\n",
        "    ellipse_annotator = sv.EllipseAnnotator(\n",
        "        color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
        "        thickness=2\n",
        "    )\n",
        "    label_annotator = sv.LabelAnnotator(\n",
        "        color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
        "        text_color=sv.Color.from_hex('#000000'),\n",
        "        text_position=sv.Position.BOTTOM_CENTER\n",
        "    )\n",
        "    triangle_annotator = sv.TriangleAnnotator(\n",
        "        color=sv.Color.from_hex('#FFD700'),\n",
        "        base=20, height=17\n",
        "    )\n",
        "\n",
        "    tracker = sv.ByteTrack()\n",
        "    tracker.reset()\n",
        "\n",
        "    frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "\n",
        "    frames_dict = {}\n",
        "\n",
        "    for frame_index, frame in enumerate(frame_generator):\n",
        "        # ball, goalkeeper, player, referee detection\n",
        "\n",
        "        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
        "        detections = sv.Detections.from_inference(result)\n",
        "\n",
        "        ball_detections = detections[detections.class_id == BALL_ID]\n",
        "        ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
        "\n",
        "        all_detections = detections[detections.class_id != BALL_ID]\n",
        "        all_detections = all_detections.with_nms(threshold=0.5, class_agnostic=True)\n",
        "        all_detections = tracker.update_with_detections(detections=all_detections)\n",
        "\n",
        "        goalkeepers_detections = all_detections[all_detections.class_id == GOALKEEPER_ID]\n",
        "        players_detections = all_detections[all_detections.class_id == PLAYER_ID]\n",
        "        referees_detections = all_detections[all_detections.class_id == REFEREE_ID]\n",
        "\n",
        "        # team assignment\n",
        "\n",
        "        players_crops = [sv.crop_image(frame, xyxy) for xyxy in players_detections.xyxy]\n",
        "        players_detections.class_id = team_classifier.predict(players_crops)\n",
        "\n",
        "        goalkeepers_detections.class_id = resolve_goalkeepers_team_id(\n",
        "            players_detections, goalkeepers_detections)\n",
        "\n",
        "        referees_detections.class_id -= 1\n",
        "\n",
        "        all_detections = sv.Detections.merge([\n",
        "            players_detections, goalkeepers_detections, referees_detections])\n",
        "\n",
        "        # frame visualization\n",
        "\n",
        "        labels = [\n",
        "            f\"#{tracker_id}\"\n",
        "            for tracker_id\n",
        "            in all_detections.tracker_id\n",
        "        ]\n",
        "\n",
        "        all_detections.class_id = all_detections.class_id.astype(int)\n",
        "\n",
        "        annotated_frame = frame.copy()\n",
        "        annotated_frame = ellipse_annotator.annotate(\n",
        "            scene=annotated_frame,\n",
        "            detections=all_detections)\n",
        "        annotated_frame = label_annotator.annotate(\n",
        "            scene=annotated_frame,\n",
        "            detections=all_detections,\n",
        "            labels=labels)\n",
        "        annotated_frame = triangle_annotator.annotate(\n",
        "            scene=annotated_frame,\n",
        "            detections=ball_detections)\n",
        "\n",
        "        players_detections = sv.Detections.merge([\n",
        "            players_detections, goalkeepers_detections\n",
        "        ])\n",
        "\n",
        "        # detect pitch key points\n",
        "\n",
        "        result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
        "        key_points = sv.KeyPoints.from_inference(result)\n",
        "\n",
        "        # project ball, players and referies on pitch\n",
        "\n",
        "        filter = key_points.confidence[0] > 0.5\n",
        "        frame_reference_points = key_points.xy[0][filter]\n",
        "        pitch_reference_points = np.array(CONFIG.vertices)[filter]\n",
        "\n",
        "        transformer = ViewTransformer(\n",
        "            source=frame_reference_points,\n",
        "            target=pitch_reference_points\n",
        "        )\n",
        "\n",
        "        frame_ball_xy = ball_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
        "        pitch_ball_xy = transformer.transform_points(points=frame_ball_xy)\n",
        "\n",
        "        players_xy = players_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
        "        pitch_players_xy = transformer.transform_points(points=players_xy)\n",
        "\n",
        "        referees_xy = referees_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
        "        pitch_referees_xy = transformer.transform_points(points=referees_xy)\n",
        "\n",
        "        # visualize video game-style radar view\n",
        "\n",
        "        annotated_frame = draw_pitch(CONFIG)\n",
        "        annotated_frame = draw_points_on_pitch(\n",
        "            config=CONFIG,\n",
        "            xy=pitch_ball_xy,\n",
        "            face_color=sv.Color.WHITE,\n",
        "            edge_color=sv.Color.BLACK,\n",
        "            radius=10,\n",
        "            pitch=annotated_frame)\n",
        "        annotated_frame = draw_points_on_pitch(\n",
        "            config=CONFIG,\n",
        "            xy=pitch_players_xy[players_detections.class_id == 0],\n",
        "            face_color=sv.Color.from_hex('00BFFF'),\n",
        "            edge_color=sv.Color.BLACK,\n",
        "            radius=16,\n",
        "            pitch=annotated_frame)\n",
        "        annotated_frame = draw_points_on_pitch(\n",
        "            config=CONFIG,\n",
        "            xy=pitch_players_xy[players_detections.class_id == 1],\n",
        "            face_color=sv.Color.from_hex('FF1493'),\n",
        "            edge_color=sv.Color.BLACK,\n",
        "            radius=16,\n",
        "            pitch=annotated_frame)\n",
        "        annotated_frame = draw_points_on_pitch(\n",
        "            config=CONFIG,\n",
        "            xy=pitch_referees_xy,\n",
        "            face_color=sv.Color.from_hex('FFD700'),\n",
        "            edge_color=sv.Color.BLACK,\n",
        "            radius=16,\n",
        "            pitch=annotated_frame)\n",
        "\n",
        "        frames_dict[frame_index] = annotated_frame\n",
        "\n",
        "    print(\"Annotated frames dumped. Turning these into GIF\")\n",
        "\n",
        "    frame_indexes = sorted(frames_dict.keys())\n",
        "    frames = [PIL_Image.fromarray(frames_dict[i]) for i in frame_indexes]\n",
        "\n",
        "    frames[0].save(\n",
        "        f\"{SOURCE_VIDEO_PATH}_radar.gif\",\n",
        "        save_all=True,\n",
        "        append_images=frames[1:],\n",
        "        duration=50,  # ms per frame\n",
        "        loop=0\n",
        "    )\n"
      ],
      "metadata": {
        "id": "TNGTF-8lOfVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ðŸ•¸ï¸ Display all generated radar plot GIFs\n",
        "\n",
        "for file in os.listdir():\n",
        "    if file.endswith(\".gif\"):\n",
        "        print(f\"Displaying GIF: {file}\")\n",
        "        display(Image(filename=file))"
      ],
      "metadata": {
        "id": "FfhJ63jFR-l3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFMa2ER_JUtG",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ðŸª© Ball tracking\n",
        "SOURCE_VIDEO_PATH = \"/content/121364_0.mp4\"\n",
        "BALL_ID = 0\n",
        "MAXLEN = 5\n",
        "\n",
        "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
        "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "\n",
        "path_raw = []\n",
        "M = deque(maxlen=MAXLEN)\n",
        "\n",
        "for frame in tqdm(frame_generator, total=video_info.total_frames):\n",
        "\n",
        "    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
        "    detections = sv.Detections.from_inference(result)\n",
        "\n",
        "    ball_detections = detections[detections.class_id == BALL_ID]\n",
        "    ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
        "\n",
        "    result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
        "    key_points = sv.KeyPoints.from_inference(result)\n",
        "\n",
        "    filter = key_points.confidence[0] > 0.5\n",
        "    frame_reference_points = key_points.xy[0][filter]\n",
        "    pitch_reference_points = np.array(CONFIG.vertices)[filter]\n",
        "\n",
        "    transformer = ViewTransformer(\n",
        "        source=frame_reference_points,\n",
        "        target=pitch_reference_points\n",
        "    )\n",
        "    M.append(transformer.m)\n",
        "    transformer.m = np.mean(np.array(M), axis=0)\n",
        "\n",
        "    frame_ball_xy = ball_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
        "    pitch_ball_xy = transformer.transform_points(points=frame_ball_xy)\n",
        "\n",
        "    path_raw.append(pitch_ball_xy)\n",
        "\n",
        "path = [\n",
        "    np.empty((0, 2), dtype=np.float32) if coorinates.shape[0] >= 2 else coorinates\n",
        "    for coorinates\n",
        "    in path_raw\n",
        "]\n",
        "\n",
        "path = [coorinates.flatten() for coorinates in path]\n",
        "\n",
        "annotated_frame = draw_pitch(CONFIG)\n",
        "annotated_frame = draw_paths_on_pitch(\n",
        "    config=CONFIG,\n",
        "    paths=[path],\n",
        "    color=sv.Color.WHITE,\n",
        "    pitch=annotated_frame)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWksgzo4q5cr",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ› Same but with outliers fixed\n",
        "\n",
        "def replace_outliers_based_on_distance(\n",
        "    positions: List[np.ndarray],\n",
        "    distance_threshold: float\n",
        ") -> List[np.ndarray]:\n",
        "    last_valid_position: Union[np.ndarray, None] = None\n",
        "    cleaned_positions: List[np.ndarray] = []\n",
        "\n",
        "    for position in positions:\n",
        "        if len(position) == 0:\n",
        "            # If the current position is already empty, just add it to the cleaned positions\n",
        "            cleaned_positions.append(position)\n",
        "        else:\n",
        "            if last_valid_position is None:\n",
        "                # If there's no valid last position, accept the first valid one\n",
        "                cleaned_positions.append(position)\n",
        "                last_valid_position = position\n",
        "            else:\n",
        "                # Calculate the distance from the last valid position\n",
        "                distance = np.linalg.norm(position - last_valid_position)\n",
        "                if distance > distance_threshold:\n",
        "                    # Replace with empty array if the distance exceeds the threshold\n",
        "                    cleaned_positions.append(np.array([], dtype=np.float64))\n",
        "                else:\n",
        "                    cleaned_positions.append(position)\n",
        "                    last_valid_position = position\n",
        "\n",
        "    return cleaned_positions\n",
        "\n",
        "MAX_DISTANCE_THRESHOLD = 500\n",
        "\n",
        "path = replace_outliers_based_on_distance(path, MAX_DISTANCE_THRESHOLD)\n",
        "\n",
        "annotated_frame = draw_pitch(CONFIG)\n",
        "annotated_frame = draw_paths_on_pitch(\n",
        "    config=CONFIG,\n",
        "    paths=[path],\n",
        "    color=sv.Color.WHITE,\n",
        "    pitch=annotated_frame)\n",
        "\n",
        "sv.plot_image(annotated_frame)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}